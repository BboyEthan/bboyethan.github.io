---
title: "[Interview]2018秋招记录"
layout: post
date: 2018-05-25 17:01
image: /assets/images/markdown.jpg
headerImage: false
tag:
- Interview
- Record
category: blog
author: ethan
description: 面试中遇到的问题及试题整理

---

# 2018秋招记录

该博客为了记录2018秋招的关键时间点、笔试和面试中遇到的问题。*keep hungry,keep foolish*。希望等年底回头来看，不负自己。

## 关键事件点

__2018.7.30__ 
- [ ] ：简历修改，毕业学长（BAT大佬）帮忙修改
- [ ] ：5家公司投递。*唯品会，贝壳，顺丰科技，拼多多。*
- [ ] : 

## 公司进度

| 名称             |  途径  |  笔试  |  面试  |  HR面 |  Offer |
|:---------------:|:-----:|:-----:|:-----:|:-----:|:-----:|
| 唯品会           |   内推    |       |       |       |       |
| 贝壳找房         |    内推   |       |       |       |       |
| 顺丰科技         |    内推   |       |       |       |       |
| 拼多多           |    内推   |  8.5  |    挂   |       |       |
| 网易云音乐       |   内推   |   8.11    |   挂    |       |       |
| 	苏宁           |	内推  |          |        |       |      |
|  Keep          |  内推  |          |        |       |      |
|  远景智能        |  内推  |         |        |       |      |
|  阿里           |  内推  | 无笔试   | 一面8.8二面8.9三面8.13|   |     |
| 依云科技        |  网申  |  无笔试   | 一面8.18  |        |      |
| 爱奇艺         |  内推  |          |          |         |      |
| 百度           |  内推  |         |           |         |      |
| 美团点评        |  内推  |         |          |          |      |
| 搜狐           |  内推  |    挂     |          |          |     |  | 知乎           | 网申   |           |         |         |      |
| 百度金融        | 网申  |            |         |         |      |
|平安科技        |网申   |            |          |         |      |
|华为       |网申   |            |          |         |      |
|滴滴       |网申   |            |          |         |      |
|英特尔       |网申   |            |          |         |      |
|蘑菇街       |网申   |            |          |         |      |
	
__简历中待修改的地方__
- [x] 竞赛名词写的具体里如（2/1000）
- [x] 每一个比赛项目写出具体解决了什么问题，算法的insight是什么。不要写个人或者团队参赛。
- [x] 比较Top前几的算法，个人算法的劣势是哪些，他们是如何达到好的名次的。
- [x] 将每个实习项目和科研项目具体落地到业务中去，体现具体完成了什么价值。


___知识点___
- [x] 常见的深度学习算法优劣（CNN&RNN），具体每种算法为什么好。	- [ ] *CNN*:权值共享使得模型更加简单，泛化能力更强；（2）局部感知连接使得对feature的抽象过程大大减少了对空间相关性对依赖，使得模型对样本的畸变不敏感（如旋转、扭曲等）。（3）池化运算可以降低网络的空间分辨率，从而消除信号的微小偏移和扭曲，从而对输入数据的平移不变性要求不高。其他：优点在网络的输入使多维图像上表现的更为明显，使图像可以直接作为网络的输入，避免了传统识别算法中复杂的特征提取和数据重建过程。
	- [ ] *RNN*:信息彼此间有着复杂的时间关联性。其关键之处在于当前网络的隐藏状态会保留先前的输入信息，用来作当前网络的输出	。许多任务需要处理序列数据，比如语音处理，人机对话，文本处理等都要求模型的输入是序列数据。
- [x] *激活函数都有哪些，优劣*
	- [ ] sigmoid:在特征相差比较复杂或是相差不是特别大时效果比较好。缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法。sigmoid单元在反向传播时，很容易就会出现梯度消失。
	- [ ] tanh:在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果。
	- [ ] RELU:1）单侧抑制 2）相对宽阔的兴奋边界 3）稀疏激活性。不存在sigmoid梯度消失的问题。速度快，只需要计算max。存在神经元死亡，即参数反向调整调整过大。需要设置为较小的神经元。
	- 比较RELU,Leaky RELU,RRelu，一个是恒为0，一个是加入一个负值的斜率，一个是训练的斜率。![contrast](https://ws3.sinaimg.cn/large/0069RVTdly1ftvn38fcsrj30kk061q37.jpg)
- [ ] *优化算法*
	- [ ] Adam
	- [ ] SGD
- [ ] 手撕经典算法
	- [x] logistic regression  	
- [ ] 集成学习都有哪些
6. Tensorflow框架都有哪些好处。GPU用了多少。
7. XGB，GBDT,SVM手撕
11. 优化算法都是哪些,SGB,Adam，等
12. *L0,L1,L2正则化* [求解方法](https://blog.csdn.net/jiajia_wu/article/details/80265980)
13. *梯度消失与梯度爆炸*（过多层，权值过大）
14. [Batch Norm](https://www.cnblogs.com/guoyaohua/p/8724433.html)
15. *深度学习overfit的处理方法*（1.参数范数惩罚 2.增加数据 3.提前终止 4.dropout 5.Batch Normalization）
16. *树模型的不同*ID3，C4.5,CART树的不同（ID3利用信息增益，C4.5利用增益率：当属性可选值过多时，信息增益过大。CART树利用GINI系数来划分）[差异](https://blog.csdn.net/qq_27717921/article/details/74784400) 
	 - 离散化
	 - 为什么xgboost要求二阶导数
		- 泰勒展开逼近残差，二阶比一阶精度高
		- 通用性，所有二阶可导的loss function都可以用
		- 有些函数梯度在一阶上变化小，二阶变化大，参考牛顿法
17. AlexNet、GoogleNet、Inception V3、 ResNet进行了详细介绍，还说了各自的优缺点和参数细节
18. *数据增强的方法* 水平翻转 随机裁剪、平移变换，颜色、光照变换
19. 随机森林减少方差，XGB减少偏差。
20. *Adam* 与SGD的核心区别在于计算更新步长时，增加了分母：梯度平方累积和的平方根。
21. [*熵，交叉熵，KL散度*](https://www.cnblogs.com/silent-stranger/p/7987708.html)

__评价函数__
1.AUC
2.ROC
3.*查准率和召回率*

精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是对的。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)。
而召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。
在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR。ROC 曲线可以用于评价一个分类器好坏。
AUC ：随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是 AUC 值。

__在线写代码__
1. 线性回归
2. 逻辑回归
3. K-means
4. 决策树

__面经中的leetcode__
1.堆排序来求数组的第K大或者小元素
## 网上面经及生疏的题目

___大疆___	

1. BP算法推导
2. 写出Leaky-Relu的数学公式，相对于sigmoid函数的有点
3. Adagrad的优缺点
4. 在图像中，data augumentation有哪些方式
5. 为什么要进行数据归一化，有哪些方式

___艾耕科技___

1. 手写两层神经网络

{% python highlight %}

n_hidden_1 = 128 # 1st layer number of neurons
n_hidden_2 = 256 # 2nd layer number of neurons
num_input = 784 # 
num_classes = 10 

X = tf.placeholder("float",[None,num_input],name="input")
y = tf.placeholder("float",[None,num_classes],name="output")

weights = {
	'hidden1':tf.Variable(tf.random_normal([num_input,n_hidden_1])),
	'hidden2':tf.Variable(tf.random_normal([n_hidden1,n_hidden_2])),
	'o_layer':tf.Variable(tf.random_normal([n_hidden2,num_classes])
}

biases = {
	'b1':tf.Variable(tf.random_normal([n_hidden_1])),
	'b2': tf.Varibale(tf.random_normal([n_hidden_2])),
	'output_b':,tf.Variable(tf.random_normal([num_classes]))
}

def net(X):
	layer_1 = tf.add(tf.matmul(X,weight['hidden1']),biases['b1'])
	layer_2 = tf.add(tf.matmul(layer_1,weight['hidden2']),biases['b2'])
	out_layer = tf.add(tf.matmul(layer_2,weight['o_layer']),biases['output_b'])
	
	return out_layer

{% endhighlight %}


- [x]贝叶斯模型的代码及解释算法原理。
- [x]最大似然估计。
4. 	二分搜索与动态规划

___快手___

1. 合并K个有序数组
2. 字典的归并排序

___阿里___

1. 找出N个数据中的最大的K个数据---堆排序
2. 腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？
看到问题的想法是采用bitmap：1个字节可以表示8个整数是否出现的情况（出现则对应的位置1，否则为0），那么表示40亿个整数的情况需要40亿/8=5亿，约500M的空间.空间复杂度是O(n)+O(1);
还有更好的方法：
这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下：
又因为2^32为40亿多，所以给定一个数可能在，也可能不在其中；
这里我们把40亿个数中的每一个用32位的二进制来表示
假设这40亿个数开始放在一个文件中。

    然后将这40亿个数分成两类:
      1.最高位为0
      2.最高位为1
    并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿（这相当于折半了）；
与要查找的数的最高位比较并接着进入相应的文件再查找

    再然后把这个文件为又分成两类:
      1.次最高位为0
      2.次最高位为1
3. MySQL计算经纬度距离、获取距离范围内的数据 1KM 3KM 5KM 

__GBDT和XGBoost的区别__

1. 传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；
传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；
2. XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，放置过拟合，这也是XGBoost优于传统GBDT的一个特性；
shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；
3. 列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算；
4. 对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动 学习出它的分裂方向；
5. XGBoost工具支持并行。Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

